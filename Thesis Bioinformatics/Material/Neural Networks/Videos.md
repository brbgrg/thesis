(1) Coding a single neuron and a layer: [https://lnkd.in/gfSRKuxt](https://lnkd.in/gfSRKuxt)  
(2) The beauty of numpy and the dot product in coding neurons and layers: [https://lnkd.in/grBjwTu4](https://lnkd.in/grBjwTu4)  
(3) Coding multiple neural network layers: [https://lnkd.in/gSwmEnZP](https://lnkd.in/gSwmEnZP)  
(4) Implementing the Dense Layer class in Python: [https://lnkd.in/gSEeZzTZ](https://lnkd.in/gSEeZzTZ)  
(5) Broadcasting and Array Summation in Python: [https://lnkd.in/gt9u5hca](https://lnkd.in/gt9u5hca)  
(6) Coding Neural Network Activation Functions from scratch: [https://lnkd.in/gxav-8-2](https://lnkd.in/gxav-8-2)  
(7) Coding one neural network forward pass: [https://lnkd.in/geyZAvAn](https://lnkd.in/geyZAvAn)  
(8) Coding the cross entropy loss in Python (from scratch): [https://lnkd.in/gbgyQbJi](https://lnkd.in/gbgyQbJi)  
(9) Introduction to Optimization in Neural Network training: [https://lnkd.in/gU2ZyXNq](https://lnkd.in/gU2ZyXNq)  
(10) Partial Derivatives and Gradient in Neural Networks: [https://lnkd.in/gmb4TgUC](https://lnkd.in/gmb4TgUC)  
(11) Understand Chain Rule-The backbone of Neural Networks: [https://lnkd.in/gpWqaB2s](https://lnkd.in/gpWqaB2s)  
(12) Backpropagation from scratch on a single neuron: [https://lnkd.in/gPXNvxwG](https://lnkd.in/gPXNvxwG)  
(13) Backpropagation through an entire layer of neurons - from scratch: [https://lnkd.in/gpTcyz3G](https://lnkd.in/gpTcyz3G)  
(14) Role of matrices in backpropagation: [https://lnkd.in/gME4Ey53](https://lnkd.in/gME4Ey53)  
(15) Finding derivatives of inputs in backpropagation and why we need them: [https://lnkd.in/gyDsmmkS](https://lnkd.in/gyDsmmkS)  
(16) Coding Backpropagation building blocks in Python: [https://lnkd.in/gA75tWfz](https://lnkd.in/gA75tWfz)  
(17) Backpropagation on the ReLU activation class: [https://lnkd.in/gaTgYZGa](https://lnkd.in/gaTgYZGa)  
(18) Implementing backpropagation on the cross entropy loss function: [https://lnkd.in/gHdFwJBf](https://lnkd.in/gHdFwJBf)  
(19) Combined backpropagation on softmax activation and cross entropy loss: [https://lnkd.in/gNJMrCX3](https://lnkd.in/gNJMrCX3)  
(20) Build the entire backpropagation pipeline for neural networks | No PyTorch or Tensorflow| Only Numpy: [https://lnkd.in/gqqmb8AN](https://lnkd.in/gqqmb8AN)  
(21) Coding the entire neural network forward backward pass in Python: [https://lnkd.in/grmHYnbn](https://lnkd.in/grmHYnbn)  
(22) Learning Rate Decay in Neural Network Optimization: [https://lnkd.in/gCbciAu9](https://lnkd.in/gCbciAu9)  
(23) Momentum in training neural networks: [https://lnkd.in/gZwFz46b](https://lnkd.in/gZwFz46b)  
(24) Coding the ADAGRAD optimizer for Neural Network training: [https://lnkd.in/gmyarquq](https://lnkd.in/gmyarquq)  
(25) Coding the RMSProp Optimizer with Neural Network training: [https://lnkd.in/gryU7Rsw](https://lnkd.in/gryU7Rsw)  
(26) Coding the ADAM optimizer for neural networks: [https://lnkd.in/grn2V7Yg](https://lnkd.in/grn2V7Yg)